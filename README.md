# Memory-efficient FFN/MoE Inference by Approximation
A sparse and retraining-free FFN/MoE inference algorithm for large language model (LLM)


## Dependencies

The project was developed with:

* `torch==2.2.1`
* `transformers==4.38.2`
* `datasets==2.18.0` 
* `lm_eval==0.4.0`
* CUDA 12.2 GPU driver

## Usage

TODO

